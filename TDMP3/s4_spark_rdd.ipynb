{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c24f43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Procesamiento distribuído con Spark\n",
    "\n",
    "<img src=\"./images/apachesparklogo.png\" alt=\"spark\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8ab8a",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "##  Inicializando Spark\n",
    "\n",
    "Lo primero que debe hacer un programa Spark es crear un objeto `SparkSession`. Se introdujo en Spark 2.0 \n",
    "y se convirtió en un punto de entrada de cada aplicación Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12b4899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/18 17:31:16 WARN Utils: Your hostname, danrec-HP-Pavilion-Gaming-Laptop-15-ec0xxx resolves to a loopback address: 127.0.1.1; using 10.8.11.97 instead (on interface wlo1)\n",
      "23/10/18 17:31:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/18 17:31:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark                                 # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e15463",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "El objeto `sparkContext` es la variable de `SparkSession` que se utiliza para operar con RDDs.\n",
    "Más adelante veremos que el objeto `SQLContext` es la variable  de `SparkSession` que se utiliza para operar con `DataFrames`  y `datasets`.\n",
    "<img src=\"./images/sparkSession.PNG\" alt=\"spark\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8448204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDD (Resilient Distributed Dataset)\n",
    "\n",
    "Los RDDs son estructuras de datos fundamentales de Apache Spark. Un RDD es una colección __inmutable__ de datos distribuidos que pueden ser manipulados en paralelo (en los diferentes nodos del clúster).\n",
    "\n",
    "* __Resilient__: tolerante a fallos. Poseen autorecuperación en caso de fallos. \n",
    "* __Distributed__: los datos en RDD se dividen lógicamente en muchos servidores para que puedan computarse en diferentes nodos del clúster.\n",
    "* __Dataset__: El conjunto de datos representa registros de los datos con los que trabaja. \n",
    "\n",
    "### Características principales\n",
    "\n",
    "* __In-memory Computation__ (Computación en memoria)\n",
    "\n",
    "Spark RDD almacena resultados intermedios en memoria distribuida (RAM) en lugar de almacenamiento estable (disco).\n",
    "\n",
    "* __Lazy Evaluations__ (Evaluación perezosas)\n",
    "\n",
    "Todas las transformaciones en Apache Spark son perezosas, ya que no calculan sus resultados de inmediato. Spark calcula las transformaciones cuando una acción requiere un resultado.\n",
    "\n",
    "* __Fault Tolerance__ (Tolerancia a fallos)\n",
    "\n",
    "Los Spark RDD son tolerantes a fallos ya que rastrean la procedencia de los datos para reconstruir automáticamente la información perdida en caso de fallo. Cada RDD recuerda cómo se creó a partir de otros conjuntos de datos para recrearse a sí mismo.\n",
    "\n",
    "* __Immutability__ (Inmutabilidad)\n",
    "\n",
    "Los datos son seguros para compartir entre procesos. Por lo tanto, es una forma de alcanzar la consistencia en los cálculos.\n",
    "\n",
    "* __Partitioning__ (Particionamiento)\n",
    "\n",
    "La partición es la unidad fundamental de paralelismo en Spark RDD. Cada partición es una división lógica de datos que es mutable. \n",
    "\n",
    "* __Persistence__ (Persistencia)\n",
    "\n",
    "Los usuarios pueden indicar qué RDD reutilizarán y elegir una estrategia de almacenamiento para ellos (por ejemplo, almacenamiento en memoria o en disco).\n",
    "\n",
    "\n",
    "###  Creación de RDD's\n",
    "\n",
    "El usuario puede cargar el conjunto de datos externamente, que puede ser un archivo JSON, un archivo CSV, un archivo de texto o una base de datos a través de JDBC sin una estructura de datos específica.\n",
    "\n",
    "#### Paralelize\n",
    "Para crear un objeto RDD podemos usar el método `sc.parallelize()`. Este método toma como argumento una colección u objeto iterable y devuelve un objeto de tipo RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144dd8e4",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "<!--\n",
    "https://data-flair.training/blogs/spark-rdd-tutorial/\n",
    "\n",
    "Hay tres formas de crear RDD en Spark, como: datos en almacenamiento estable, otros RDD y paralelizar la recopilación ya existente en el programa del controlador. También se pueden operar Spark RDD en paralelo con una API de bajo nivel que ofrece transformaciones y acciones. Estudiaremos estas operaciones Spark RDD más adelante en esta sección.\n",
    "\n",
    "Spark RDD también se puede almacenar en caché y particionar manualmente. El almacenamiento en caché es beneficioso cuando usamos RDD varias veces. Y la partición manual es importante para equilibrar correctamente las particiones. Generalmente, las particiones más pequeñas permiten distribuir los datos RDD de manera más equitativa, entre más ejecutores. Por lo tanto, menos particiones facilitan el trabajo.\n",
    "\n",
    "Los programadores también pueden llamar a un método de persistencia para indicar qué RDD quieren reutilizar en operaciones futuras. Spark mantiene los RDD persistentes en la memoria de forma predeterminada, pero puede volcarlos al disco si no hay suficiente RAM. Los usuarios también pueden solicitar otras estrategias de persistencia, como almacenar el RDD solo en el disco o replicarlo entre máquinas, a través de indicadores para persistir.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos primero el contexto.  Un objeto  SparkContext representa la conexión \n",
    "# con el cluster Spark cluster, y puede ser usado para crear RDDs\n",
    "sc = spark.sparkContext\n",
    "array = sc.parallelize([1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf3e11",
   "metadata": {},
   "source": [
    "Podemos preguntar por el tipo de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a9fc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aaac3a",
   "metadata": {},
   "source": [
    "Los elementos de la colección son copiados a un objeto distribuido sobre el cual se puede operar de forma distribuída.  \n",
    "\n",
    "\n",
    "En realidad, dicho método solo le dice a Spark que distribuya los datos __pero no se realiza hasta que no se ejecute alguna acción__. Entre las acciones posibles tenemos: \n",
    "`count`, `collect`, `take`, `reduce`. etc.\n",
    "\n",
    "Por ejemplo, si queremos sumar todos los elementos del objeto `array`, escribiremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffec1838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c8d8",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "__Particiones__\n",
    "\n",
    "Un parámetro importante de la operación `parallelize` es la cantidad de particiones en las que se cortará el conjunto de datos. Normalmente, Spark intenta establecer la cantidad de particiones automáticamente en función del clúster. Sin embargo, es posible configurarlo manualmente. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52886f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = sc.parallelize([1, 2, 3, 4, 5, 6, 7], 3)\n",
    "array.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc7a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aaaf50",
   "metadata": {},
   "source": [
    "#### Datos externos\n",
    "\n",
    "Los objetos RDD también pueden ser creados a partir de ficheros locales, y de cualquier fuente de almacenamiento compatible (HDFS, HBase, etc.). \n",
    "\n",
    "Para convertir un archivo de texto en un RDD, usamos el método `textFile()`.\n",
    "\n",
    "__Ejemplo 1__ (Fichero CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c955f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/Bus_Breakdown_and_Delays.csv\"\n",
    "\n",
    "# así leeríamos un dataframe        \n",
    "#datos = spark.read.csv(path,sep=';', header=True)\n",
    "# pero de momento leemos un RDD \n",
    "datos = sc.textFile(path) \n",
    "print(datos.count())\n",
    "type(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198291a",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El método `textFile` devuelve un registro por cada línea del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9995f",
   "metadata": {},
   "source": [
    "__Ejemplo 2__ (Fichero txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eb594",
   "metadata": {},
   "source": [
    "El fichero ['data/shakespeare.txt'](data/shakespeare.txt) contiene las Obras completas de William Shakespeare del [Proyecto Gutenberg](http://www.gutenberg.org/wiki/Página_principal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0297bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de líneas: 122395\n",
      "Número de particiones: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fileName =  'data/shakespeare.txt'\n",
    "\n",
    "# Creamos un RDD de Spark\n",
    "shakespeareRDD = sc.textFile(fileName, 8)\n",
    "print(f\"Número de líneas: {shakespeareRDD.count()}\")\n",
    "print (f'Número de particiones: {shakespeareRDD.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5381302",
   "metadata": {},
   "source": [
    "<!--\n",
    "De forma predeterminada, cada RDD transformado se puede volver a calcular cada vez que ejecuta una acción en él. Sin embargo, también puede conservar un RDD en la memoria usando el método persistente (o caché), en cuyo caso Spark mantendrá los elementos en el clúster para un acceso mucho más rápido la próxima vez que lo consulte. También hay soporte para RDD persistentes en el disco o replicados en múltiples nodos.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e7f9c",
   "metadata": {},
   "source": [
    "## Operaciones con RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2d92a",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los RDD admiten dos tipos de operaciones:\n",
    "<img src=\"./images/operaciones_rdd.PNG\" alt=\"spark\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "### Transformaciones\n",
    "\n",
    "Son operaciones que __producen un nuevo RDD__ a partir de los RDD existentes. Toma RDD como entrada y produce uno o más RDD como salida. Los RDD de entrada no se pueden cambiar ya que los RDD son de naturaleza inmutable. Las transformaciones son las que crean el registro de procedencia de los RDD (linaje). Esto es un grafo acíclico dirigido.\n",
    "\n",
    "Las transformaciones son de naturaleza perezosa, es decir, se ejecutan cuando llamamos a una acción. No se ejecutan inmediatamente. Este diseño permite que Spark funcione de manera más eficiente. \n",
    "\n",
    "Por ejemplo, la operación `map` es una transformación que pasa cada elemento del conjunto de datos a través de una función y devuelve un nuevo RDD que representa los resultados.\n",
    "\n",
    "### Acciones\n",
    "\n",
    "Los executers ejecutan un cálculo en el conjunto de datos y __devuelven un valor__ (resultado no RDD) al programa controlador. Las acciones ponen en marcha la pereza de RDD.\n",
    "\n",
    "La operación `reduce` es una acción que agrega todos los elementos del RDD usando alguna función y devuelve el resultado final al programa controlador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb483f",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "__Ejemplo:__\n",
    "\n",
    "Queremos contar el número de caracteres del fichero [data/shakespeare.txt](data/shakespeare.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556666d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de caracteres en el fichero:  5205583\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('data/shakespeare.txt')\n",
    "lineLengths = lines.map(lambda s: len(s))                  # contamos los caracteres de cada línea\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)       # sumamos los caracteres de cada línea\n",
    "\n",
    "print(f'Numero de caracteres en el fichero:  {totalLength}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da90f3",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "* La primera línea define un RDD a partir de un fichero. Este conjunto de datos no se carga en la memoria: las líneas son simplemente un puntero al archivo.\n",
    "* La segunda línea define el objeto `lineLengths` como resultado de una transformación `map`. Nuevamente, `lineLengths` no se calcula de inmediato debido a la pereza.\n",
    "* Finalmente, ejecutamos la acción `reduce`, que es una acción. En este punto, Spark divide el cálculo en tareas para ejecutarlas en máquinas separadas, y cada máquina ejecuta tanto el `map` como  el `reduce` sobre la partición local, devolviendo solo su respuesta al programa controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1be47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.rdd.RDD, pyspark.rdd.PipelinedRDD, int)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines), type(lineLengths), type(totalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1eb27a",
   "metadata": {},
   "source": [
    "### Acciones (RDD -> Resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc86273",
   "metadata": {},
   "source": [
    "#### Take\n",
    "\n",
    "Acción encargada de  seleccionar una cantidad `N` de elementos y devolverlos al driver en formato de lista. Debemos asegurarnos de que caben en memoria.\n",
    "\n",
    "<img src=\"./images/take.PNG\" alt=\"spark\" style=\"width: 300px;\"/>\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7986502e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostrar los 2 primeros elementos\n",
    "datos = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "datos.take(2)                                       # devuelve al drive 2 elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b3eaf",
   "metadata": {},
   "source": [
    "__Otras variantes__\n",
    "\n",
    "* `takeSample`:Devuelve una muestra aleatoria\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d917a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Devuleve 3 elementos aleatorios sin reemplazo\n",
    "datos = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "datos.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9916376",
   "metadata": {},
   "source": [
    "#### Count\n",
    "\n",
    "Operación que cuenta el número de elementos en el RDD.\n",
    "\n",
    "__Ejemplo__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864eab9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos = sc.parallelize([10, 20, 30])\n",
    "datos.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3eda7",
   "metadata": {},
   "source": [
    "Tiene otras variantes:\n",
    "\n",
    "- `countApprox(timeout, confidence=0.95)`: cuenta el número aproximado de valores (más rápido). Cuanto mayor sera `confidence` mayor exactitud y más lento.\n",
    "\n",
    "- `countApproxDistinct(relativeSD=0.05)`: número aproximado de valores distintos. El valor `relativeSD` indica la precisión: cuanto más pequeño, más exactitud y más lento.\n",
    "\n",
    "- `countByValue`: cuenta el número de repeticiones de cada elemento. Devuelve un dicionario.\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f8e68be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 2: 3})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = sc.parallelize([1, 2, 1, 2, 2])\n",
    "m = lista.countByValue()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dcc35b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74dd30",
   "metadata": {},
   "source": [
    "- `countByKey` : Cuenta el número de valores en cada clave, válido para RDDs de la forma (clave,valor)\n",
    "\n",
    "\n",
    "<img src=\"./images/countbykey.PNG\" alt=\"spark\" style=\"width: 300px;\"/>\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf9ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 3)])\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e584cc1",
   "metadata": {},
   "source": [
    "#### Collect\n",
    "\n",
    "Esta operación devuelve al driver el RDD al completo como una lista. Equivale a:\n",
    "```\n",
    "rdd.take(rdd.count())\n",
    "```\n",
    "Debe utilizarse con precaución, solo para RDDs pequeños.\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "513228ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = sc.parallelize([1, 2, 3, 4, 5])\n",
    "lista.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa2353",
   "metadata": {},
   "source": [
    "#### Reduce\n",
    "\n",
    "Aggrega todos los elementos de un RDD aplicando un operador binario y conmutativo de forma recurrente a todo el RDD.\n",
    "\n",
    "Por ejemplo:\n",
    "```\n",
    "     reduce( + , [a,b,c,d]) = a + b + c + d \n",
    "```     \n",
    "Hay que tener en cuenta que la operación se puede realizar sobre  una \"permutación\" de la lista.\n",
    "\n",
    "<img src=\"./images/reduce.PNG\" alt=\"spark\" style=\"width: 300px;\"/>\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a31390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = sc.parallelize([1, 2, 3, 4])\n",
    "lista.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb81b47",
   "metadata": {},
   "source": [
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06d06af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcula_minimo(a, b):\n",
    "    return min(a,b)\n",
    "\n",
    "lista = sc.parallelize([7, 2, 3, 4])  # la lista no debe ser vacía\n",
    "lista.reduce(calcula_minimo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28766ab3",
   "metadata": {},
   "source": [
    "También es posible usar las funciones de agregación del módulo  `operator`.  \n",
    "[https://docs.python.org/3/library/operator.html](https://docs.python.org/3/library/operator.html)\n",
    "\n",
    "<table class=\"docutils align-default\">\n",
    "<colgroup>\n",
    "<col style=\"width: 26%\">\n",
    "<col style=\"width: 29%\">\n",
    "<col style=\"width: 45%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Operación</p></th>\n",
    "<th class=\"head\"><p>Sintaxis Python</p></th>\n",
    "<th class=\"head\"><p> Funciones en el módulo  operator</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>Addition</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">+</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">add(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Concatenation</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">seq1</span> <span class=\"pre\">+</span> <span class=\"pre\">seq2</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">concat(seq1,</span> <span class=\"pre\">seq2)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Containment Test</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">obj</span> <span class=\"pre\">in</span> <span class=\"pre\">seq</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">contains(seq,</span> <span class=\"pre\">obj)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Division</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">/</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">truediv(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Division</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">//</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">floordiv(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Bitwise And</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">&amp;</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">and_(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Bitwise Exclusive Or</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">^</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">xor(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Bitwise Or</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">|</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">or_(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Exponentiation</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">**</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">pow(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Identity</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">is</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">is_(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Identity</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">is</span> <span class=\"pre\">not</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">is_not(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Indexed Assignment</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">obj[k]</span> <span class=\"pre\">=</span> <span class=\"pre\">v</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">setitem(obj,</span> <span class=\"pre\">k,</span> <span class=\"pre\">v)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Indexed Deletion</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">del</span> <span class=\"pre\">obj[k]</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">delitem(obj,</span> <span class=\"pre\">k)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Indexing</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">obj[k]</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">getitem(obj,</span> <span class=\"pre\">k)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Modulo</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">%</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">mod(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Multiplication</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">*</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">mul(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Matrix Multiplication</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">@</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">matmul(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Negation (Arithmetic)</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">-</span> <span class=\"pre\">a</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">neg(a)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Negation (Logical)</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">not</span> <span class=\"pre\">a</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">not_(a)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Positive</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">+</span> <span class=\"pre\">a</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">pos(a)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Right Shift</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">&gt;&gt;</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">rshift(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Slice Assignment</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">seq[i:j]</span> <span class=\"pre\">=</span> <span class=\"pre\">values</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">setitem(seq,</span> <span class=\"pre\">slice(i,</span> <span class=\"pre\">j),</span> <span class=\"pre\">values)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Slice Deletion</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">del</span> <span class=\"pre\">seq[i:j]</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">delitem(seq,</span> <span class=\"pre\">slice(i,</span> <span class=\"pre\">j))</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Slicing</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">seq[i:j]</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">getitem(seq,</span> <span class=\"pre\">slice(i,</span> <span class=\"pre\">j))</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>String Formatting</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">s</span> <span class=\"pre\">%</span> <span class=\"pre\">obj</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">mod(s,</span> <span class=\"pre\">obj)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Subtraction</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">-</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">sub(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Ordering</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">&lt;</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">lt(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Ordering</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">&lt;=</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">le(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Equality</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">==</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">eq(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Difference</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">!=</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">ne(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Ordering</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">&gt;=</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">ge(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Ordering</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">&gt;</span> <span class=\"pre\">b</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">gt(a,</span> <span class=\"pre\">b)</span></code></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adf165ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator as op\n",
    "lista = sc.parallelize([6, 2, 1])    # la lista no debe ser vacía\n",
    "lista.reduce(op.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9a56c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista.reduce(op.truediv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589542f2",
   "metadata": {},
   "source": [
    "#### ReduceByKey\n",
    "\n",
    "Análogo a `reduce`, pero aplicando la función a los elementos que tengan la misma clave\n",
    "en parejas de la forma (clave, valor).\n",
    "\n",
    "__Ejemplo__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37d21ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 3)])\n",
    "lista.reduceByKey(op.add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51821cd6",
   "metadata": {},
   "source": [
    "### Transformaciones (RDD -> RDD')\n",
    "\n",
    "Se suele distinguir dos tipos de transformaciones:\n",
    "\n",
    "* __Transformaciones narrow__: afecta a cada partición por separado. Son `map` y `filter`. Cada partición en el RDD resultado proviene de una única partición del RDD origen.\n",
    "* __Transformaciones wider__: pueden mezclar los resultados de distintas particiones. Cada partición en el RDD resultado puede provenir de varias particiones distintas del RDD origen. Por ejemplo `groupByKey`.\n",
    "\n",
    "#### Map\n",
    "\n",
    "Operación que permite aplicar una función a todos los elementos del RDD. \n",
    "\n",
    "__Ejemplo:__\n",
    "\n",
    "El RDD `shakespeareRDD` consiste en una secuencia de strings (cada una de las líneas del fichero). Sabemos que dentro de cada string tenemos palabras. Convertir el RDD `shakespeareRDD` en un RDD de listas de palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85b3e0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DROMIO OF EPHESUS. Master, knock the door hard.',\n",
       " \"  GLOUCESTER. We are the Queen's abjects and must obey.\",\n",
       " \"    That thou might'st think upon these by the seal,\",\n",
       " '',\n",
       " '    No, prelate; such is thy audacious wickedness,']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeareRDD = sc.textFile(fileName)\n",
    "shakespeareRDD.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "968079c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['And', 'just', 'against', 'thy', 'heart', 'make', 'thou', 'a', 'hole,']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras = shakespeareRDD.map(lambda line: line.strip().split(\" \"))\n",
    "palabras.takeSample(False, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cc883",
   "metadata": {},
   "source": [
    "#### flatMap\n",
    "\n",
    "Esta operación es similar a `map`, pero con la ventaja de que aplana los resultados:\n",
    "* `map` devuelve lista de listas\n",
    "* `flatMap` devuelve una única lista  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c17ccbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thus',\n",
       " 'noble',\n",
       " \"amaz'd,\",\n",
       " 'Doctor,',\n",
       " 'for',\n",
       " 'he',\n",
       " 'point',\n",
       " 'yet',\n",
       " 'well.-',\n",
       " 'KING']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras = shakespeareRDD.flatMap(lambda line: line.strip().split(\" \"))\n",
    "palabras.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404a868",
   "metadata": {},
   "source": [
    "#### Filter\n",
    "\n",
    "La operación `filter` devuelve un nuevo RDD que contiene solo los elementos que cumplen una determinada condición. Se considera una transformación de tipo narrow los datos de una partición del RDD resultante pertenecen a una única partición del RDD origen.\n",
    "\n",
    "__Ejemplo:__\n",
    "\n",
    "Supongamos que queremos saber cuántas veces se repite la palabra 'what' en el RDD `shakespeareRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e1133f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2483"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras = shakespeareRDD.flatMap(lambda line: line.strip().split(\" \"))\n",
    "what_rdd = palabras.filter(lambda x: x == 'we')\n",
    "what_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3f62e",
   "metadata": {},
   "source": [
    "#### Group by\n",
    "\n",
    "Agrupa los datos del RDD mediante una función . Devuelve un RDD creando pares (clave, valor), donde la clave es el resultado de la función, y el valor es una lista de todos los valores del RDD cuyo resultado es dicha clave.\n",
    "\n",
    "__Ejemplo:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35572e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', <pyspark.resultiterable.ResultIterable at 0x188ce74ec20>),\n",
       " ('L', <pyspark.resultiterable.ResultIterable at 0x188ce761660>),\n",
       " ('R', <pyspark.resultiterable.ResultIterable at 0x188ce761600>)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = sc.parallelize(['Ana', 'Luis', 'Anabel', 'Raúl', 'Lola', 'Ana'])\n",
    "g = nombres.groupBy(lambda x: x[0])    # agrupamos por la inicial del nombre\n",
    "g.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f4d84fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['Ana', 'Anabel', 'Ana']\n",
      "L ['Luis', 'Lola']\n",
      "R ['Raúl']\n"
     ]
    }
   ],
   "source": [
    "for (c, v) in g.collect():\n",
    "    print(c, list(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb16bc8",
   "metadata": {},
   "source": [
    "## Otras operaciones\n",
    "\n",
    "    \n",
    "### Combinación de RDDs\n",
    "\n",
    "- [join](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.join.html): combinando RDDs. También existen leftOuterJoin y rightOuterJoin\n",
    "        \n",
    "- [intersection](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.intersection.html): intersección de RDDs\n",
    "\n",
    "- [substract](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.subtract.html): elementos de un RDD que no están contenidos en otro\n",
    "\n",
    "- [union](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.union.html): unión de RDDs (en el sentido de juntar valores, no quita repetidos)\n",
    "\n",
    "- [zip](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.zip.html): empareja dos RDDs que se supone que tienen la misma cantidad de elementos\n",
    "    \n",
    "### RDDs (clave,valor)\n",
    "\n",
    "- [contains](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkConf.contains.html): indica si el RDD contiene una clave\n",
    "\n",
    "- [lookup](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.lookup.html): busca una clave        \n",
    "- [get](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkConf.get.html): valor asociado a una clave, o un valor por defecto si no está\n",
    "\n",
    "- [keyby](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.keyBy.html): Crea tuplas\n",
    "\n",
    "- [keys](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.keys.html): las claves de las tuplas (primer elemento)\n",
    "\n",
    "- [values](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.values.html): los valores de las tuplas (segundo elemento)\n",
    "    \n",
    "### Operaciones estadísticas:\n",
    "\n",
    "- [max](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.max.html): máximo de la colección\n",
    "\n",
    "- [mean](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.mean.html): la media de la colección\n",
    "\n",
    "- [meanApprox](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.meanApprox.html): la media aproximada de la colección\n",
    "\n",
    "- [stats](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.stats.html?highlight=stats): media, varianza y recuento de elementos en un solo resultado \n",
    "        \n",
    "        \n",
    "__Ejemplo__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5543ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'L', 'R']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471064a",
   "metadata": {},
   "source": [
    "__Ejemplo:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815f8cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ana', 100), ('Luis', 200), ('Anabel', 300), ('Raúl', 400), ('Lola', 500)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = sc.parallelize(['Ana', 'Luis', 'Anabel', 'Raúl', 'Lola'])\n",
    "salarios = sc.parallelize([100,200,300,400,500])\n",
    "resultado = nombres.zip(salarios)\n",
    "resultado.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19ba257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slario máximo\n",
    "resultado.values().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93663233",
   "metadata": {},
   "source": [
    "__Ejercicio:__ Calcular los empleados con máximo salario \n",
    "<!--    \n",
    "m = resultado.filter(lambda x : x[1] == 500)\n",
    "m.collect() \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "724f60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lola', 500)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sol: [('Lola', 500)]\n",
    "rs = resultado.filter(lambda x: x[1] == 500)\n",
    "rs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156415b1",
   "metadata": {},
   "source": [
    "## Creación de dataFrames \n",
    "\n",
    "Una vez que tenemos un RDD  \"limpio\" podemos pasarlo a dataframe. Esto se hace en dos pasos:\n",
    "\n",
    "__1.-__ Convertirlo con `map` en un RDD de Rows <br>\n",
    "__2.-__ Pasar el RDD de rows a DataFrame <br>\n",
    "\n",
    "Lo veremos en la siguiente sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f944b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre='Ana', salario=100, fecha=datetime.datetime(2022, 10, 6, 17, 37, 30, 92906)),\n",
       " Row(nombre='Luis', salario=200, fecha=datetime.datetime(2022, 10, 6, 17, 37, 31, 709467)),\n",
       " Row(nombre='Anabel', salario=300, fecha=datetime.datetime(2022, 10, 6, 17, 37, 34, 539873))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "# Paso 1\n",
    "row_data = resultado.map(lambda tupla: Row(\n",
    "    nombre=tupla[0], \n",
    "    salario=tupla[1],\n",
    "    fecha=datetime.datetime.now()\n",
    "    )\n",
    ")\n",
    "row_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "024a686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|nombre|salario|               fecha|\n",
      "+------+-------+--------------------+\n",
      "|   Ana|    100|2022-10-06 17:37:...|\n",
      "|  Luis|    200|2022-10-06 17:37:...|\n",
      "|Anabel|    300|2022-10-06 17:37:...|\n",
      "|  Raúl|    400|2022-10-06 17:37:...|\n",
      "|  Lola|    500|2022-10-06 17:37:...|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 2\n",
    "df = spark.createDataFrame(row_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab38dd5",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* RDD Programming Guide https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "237.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
